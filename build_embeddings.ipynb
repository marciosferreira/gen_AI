{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.utils.export import generate_multimodal_pages\n",
    "from docling.utils.utils import create_hash\n",
    "_log = logging.getLogger(__name__)\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(\"pdfs/smart_baby_camera.pdf\")\n",
    "    output_dir = Path(\"scratch\")\n",
    "\n",
    "    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "    # will destroy them for cleaning up memory.\n",
    "    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.\n",
    "    # scale=1 correspond of a standard 72 DPI image\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    for (\n",
    "        content_text,\n",
    "        content_md,\n",
    "        content_dt,\n",
    "        page_cells,\n",
    "        page_segments,\n",
    "        page,\n",
    "    ) in generate_multimodal_pages(conv_res):\n",
    "\n",
    "        dpi = page._default_image_scale * 72\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"document\": conv_res.input.file.name,\n",
    "                \"hash\": conv_res.input.document_hash,\n",
    "                \"page_hash\": create_hash(\n",
    "                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n",
    "                ),\n",
    "                \"image\": {\n",
    "                    \"width\": page.image.width,\n",
    "                    \"height\": page.image.height,\n",
    "                    \"bytes\": page.image.tobytes(),\n",
    "                },\n",
    "                \"cells\": page_cells,\n",
    "                \"contents\": content_text,\n",
    "                \"contents_md\": content_md,\n",
    "                \"contents_dt\": content_dt,\n",
    "                \"segments\": page_segments,\n",
    "                \"extra\": {\n",
    "                    \"page_num\": page.page_no + 1,\n",
    "                    \"width_in_points\": page.size.width,\n",
    "                    \"height_in_points\": page.size.height,\n",
    "                    \"dpi\": dpi,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate one parquet from all documents\n",
    "    df = pd.json_normalize(rows)\n",
    "    now = datetime.datetime.now()\n",
    "    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n",
    "    df.to_parquet(output_filename)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(\n",
    "        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Substitua pelo caminho do arquivo Parquet gerado\n",
    "parquet_file = \"scratch/multimodal_2024-12-20_143138.parquet\"\n",
    "\n",
    "# Carregar o Parquet em um DataFrame\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Visualizar as primeiras linhas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver o texto da primeira página\n",
    "print(df['contents'][0])\n",
    "\n",
    "# Listar os textos de todas as páginas\n",
    "for idx, content in enumerate(df['contents']):\n",
    "    print(f\"Página {idx + 1}:\")\n",
    "    print(content)\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Selecionar a primeira linha do DataFrame\n",
    "image_width = df.iloc[0]['image.width']\n",
    "image_height = df.iloc[0]['image.height']\n",
    "image_bytes = df.iloc[0]['image.bytes']\n",
    "\n",
    "# Criar a imagem a partir dos bytes\n",
    "image = Image.frombytes('RGB', (image_width, image_height), image_bytes, 'raw')\n",
    "\n",
    "# Mostrar a imagem\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['contents'][0])  # Ver o texto extraído da página 2, por exemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Inicializar o splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Criar os chunks com metadados\n",
    "chunks = []\n",
    "for _, row in df.iterrows():\n",
    "    splits = text_splitter.split_text(row['contents'])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"page_num\": row['extra.page_num'],  # Metadado do número da página\n",
    "            \"document_name\": row['document']  # Nome do documento\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Carregar o modelo\n",
    "embedding_model = SentenceTransformer('sentence-transformers/gtr-t5-large')\n",
    "\n",
    "texts = [chunk['text'] for chunk in chunks]\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adicionar os embeddings aos chunks\n",
    "for chunk, embedding in zip(chunks, embeddings):\n",
    "    chunk['embedding'] = embedding.tolist()  # Converter para lista para compatibilidade com JSON/serialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Salvar os chunks em um arquivo Pickle\n",
    "with open(\"chunks_with_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(\"Chunks com embeddings salvos em 'chunks_with_embeddings.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
