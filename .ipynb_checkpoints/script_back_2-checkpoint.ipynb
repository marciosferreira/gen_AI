{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346599c-7bc6-4216-81fa-bb81b50280a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google/gemma-7b-it # meta-llama/Meta-Llama-3-8B-Instruct # google/gemma-2-27b-it\n",
    "#sudo apt-get update\n",
    "#sudo apt-get install libmagic1\n",
    "# pip install -r requirements.txt\n",
    "# pip install transformers==4.46.3\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# pip install bitsandbytes\n",
    "# pip install accelerate\n",
    "# pip install sentence-transformers\n",
    "# pip install langchain\n",
    "# pip install -U langchain-community\n",
    "# pip install unstructured\n",
    "# pip install \"unstructured[pdf]\"\n",
    "# pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e73aaf6-4e0e-43d4-aaab-93428f8a2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponível: True\n",
      "Nome da GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU disponível:\", torch.cuda.is_available())\n",
    "print(\"Nome da GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhuma GPU detectada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80e3763-5143-42df-af2c-31d2768f1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings = False  # Defina para False se quiser carregar do disco\n",
    "\n",
    "# Caminho para salvar o índice e as embeddings\n",
    "index_path = \"faiss_index.bin\"\n",
    "embeddings_path = \"embeddings.npy\"\n",
    "texts_path = \"texts.npy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c31a146-51cd-4bee-8014-d5e23a9b0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "def create_and_save_embeddings():\n",
    "    # Carregando documentos PDF\n",
    "    loader = DirectoryLoader(\"pdfs/\", glob=\"**/*.pdf\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Dividindo os documentos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Criando embeddings com HuggingFace\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/gtr-t5-large\")\n",
    "\n",
    "    # Gerar embeddings para os documentos em formato NumPy\n",
    "    docs_texts = [doc.page_content for doc in docs]\n",
    "    docs_embeddings = np.array([embeddings.embed_query(text) for text in docs_texts]).astype('float32')\n",
    "\n",
    "    # Salvar embeddings e textos em disco\n",
    "    np.save(embeddings_path, docs_embeddings)\n",
    "    np.save(texts_path, docs_texts)\n",
    "\n",
    "    # Criar índice FAISS\n",
    "    dimension = docs_embeddings.shape[1]  # Dimensão das embeddings\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)  # Usar L2 (distância euclidiana)\n",
    "    faiss_index.add(docs_embeddings)  # Adicionar as embeddings ao índice\n",
    "\n",
    "    # Salvar o índice FAISS em disco\n",
    "    faiss.write_index(faiss_index, index_path)\n",
    "\n",
    "    return docs_embeddings, docs_texts, faiss_index\n",
    "\n",
    "def load_embeddings_and_index():\n",
    "    # Carregar embeddings e textos do disco\n",
    "    docs_embeddings = np.load(embeddings_path).astype('float32')\n",
    "    docs_texts = np.load(texts_path, allow_pickle=True)\n",
    "\n",
    "    # Carregar índice FAISS do disco\n",
    "    faiss_index = faiss.read_index(index_path)\n",
    "    return docs_embeddings, docs_texts, faiss_index\n",
    "\n",
    "if create_embeddings:\n",
    "    docs_embeddings, docs_texts, faiss_index = create_and_save_embeddings()\n",
    "else:\n",
    "    docs_embeddings, docs_texts, faiss_index = load_embeddings_and_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfe2177-b323-4ed2-a390-86263698ceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:23<00:00, 95.77s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc4f31a1-887a-4afa-b645-c366f65f2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# Configurar dispositivo (CPU ou GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inicializar histórico global da conversa\n",
    "conversation_history = []\n",
    "\n",
    "# Função para buscar contexto no RAG\n",
    "def fetch_context_from_rag(query_texts, faiss_index, embeddings, docs_texts, k=3):\n",
    "    \"\"\"\n",
    "    Busca o contexto mais relevante usando FAISS e embeddings para uma lista de textos de consulta.\n",
    "\n",
    "    Args:\n",
    "    - query_texts (list): Lista de perguntas/textos de consulta do usuário.\n",
    "    - faiss_index: Índice FAISS.\n",
    "    - embeddings: Modelo de embeddings.\n",
    "    - docs_texts (list): Textos associados ao índice FAISS.\n",
    "    - k (int): Número de textos mais similares a recuperar para cada consulta.\n",
    "\n",
    "    Returns:\n",
    "    - str: Contexto concatenado com todos os textos relevantes.\n",
    "    \"\"\"\n",
    "    all_top_texts = []  # Lista para armazenar todos os textos recuperados\n",
    "\n",
    "    # Iterar sobre cada consulta na lista\n",
    "    for query_text in query_texts:\n",
    "        # Calcular as embeddings da consulta\n",
    "        query_embedding = np.array(embeddings.embed_query(query_text)).astype('float32')\n",
    "        \n",
    "        # Buscar as k embeddings mais próximas no FAISS\n",
    "        distances, indices = faiss_index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        # Recuperar os textos correspondentes e adicionar à lista\n",
    "        top_texts = [docs_texts[idx] for idx in indices[0] if idx != -1]\n",
    "        all_top_texts.extend(top_texts)\n",
    "    \n",
    "    # Concatenar todos os textos recuperados com o delimitador \"<Next>\"\n",
    "    context = \"<Next> \\n\".join(all_top_texts)\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inicializar o histórico global fora da função\n",
    "conversation_history = []\n",
    "\n",
    "# Função principal para interação com o modelo\n",
    "def interact_with_model(query_text, faiss_index, embeddings, docs_texts, pipeline):\n",
    "    \"\"\"\n",
    "    Interage com o modelo LLM utilizando o RAG para buscar contexto e mantém o histórico.\n",
    "\n",
    "    Args:\n",
    "    - query_text (str): Pergunta do usuário.\n",
    "    - faiss_index: Índice FAISS.\n",
    "    - embeddings: Modelo de embeddings.\n",
    "    - docs_texts (list): Textos associados ao índice FAISS.\n",
    "    - tokenizer: Tokenizador do modelo LLM.\n",
    "    - model_llm: Modelo de linguagem LLM.\n",
    "\n",
    "    Returns:\n",
    "    - str: Resposta gerada pelo modelo.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    # Verificar se é a primeira interação\n",
    "    first_interaction = len(conversation_history) == 0\n",
    "    if first_interaction:\n",
    "        print(\"FI: Primeira interação detectada\")\n",
    "        # Buscar contexto atualizado\n",
    "        context = fetch_context_from_rag([query_text], faiss_index, embeddings, docs_texts)\n",
    "\n",
    "        #print(context)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"You are a helpful assistant that is supposed to help customers to adjust their equipment. \"\n",
    "                    f\"Please look at the context extracted from the manual of the equipment, and use it in case it helps \"\n",
    "                    f\"to answer the customer's question <context>{context}</context>\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_text,\n",
    "            },\n",
    "                   ]        \n",
    "                       \n",
    "        conversation_history = [{\"role\": \"user\", \"content\": query_text}]\n",
    "        \n",
    "    else:\n",
    "        print(\"SI: Interação subsequente detectada\")\n",
    "        # Adicionar a nova pergunta com o contexto longo ao histórico\n",
    "        # Buscar contexto atualizado\n",
    "        # Substring a ser removida\n",
    "        \n",
    "        user_queries = [entry['content'] for entry in conversation_history if entry['role'] == 'user']\n",
    "        user_queries.append(query_text)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "        context = fetch_context_from_rag(user_queries, faiss_index, embeddings, docs_texts)\n",
    "\n",
    "        messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"You are a helpful assistant that is supposed to help customers to adjust their equipment. \"\n",
    "                f\"Please look at the context extracted from the manual of the equipment, and use it in case it helps \"\n",
    "                f\"to answer the customer's question <context>{context}</context>\"\n",
    "            ),\n",
    "        }\n",
    "                   ]\n",
    "\n",
    "        # Adicionar o histórico da conversa\n",
    "        messages.extend(conversation_history)\n",
    "    \n",
    "        # Adicionar a pergunta atual (user role)\n",
    "        messages.append({\"role\": \"user\", \"content\": query_text})  \n",
    "        \n",
    "        \n",
    "        conversation_history.append({\"role\": \"user\", \"content\": query_text})\n",
    "\n",
    "    # Chamar o modelo com o histórico atualizado\n",
    "\n",
    "    print(messages)\n",
    "    \n",
    "    outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    )\n",
    "    response = outputs[0][\"generated_text\"][-1]\n",
    "\n",
    "    # Adicionar a resposta atual (assistant role)\n",
    "    conversation_history.append(response)  \n",
    "    conversation_history = conversation_history[-6:]\n",
    "\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce34ea0f-9d42-48be-a182-90ff4d3dcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI: Primeira interação detectada\n",
      "[{'role': 'system', 'content': \"You are a helpful assistant that is supposed to help customers to adjust their equipment. Please look at the context extracted from the manual of the equipment, and use it in case it helps to answer the customer's question <context>Do not cover this product or cables with towels and blankets.\\n\\nWhen the product is in operation, it will dissipate normal heat produced to the surface of the product. Please do not leave\\n\\nthe product on your knees or any part of your body for an extended period of time to avoid possible physical discomfort due\\n\\nto high temperature.\\n\\nPlease avoid placing this product near magnetic materials (such as speakers and TVs).\\n\\nDo not use or expose this product in an environment that could potentially leak flammable gas, or expose the product to rain\\n\\nor humid conditions.\\n\\nAvoid using the modem to connect to the Internet under thunder and lightning to avoid long-distance electric shock.\\n\\nDo not store this product near a fire source or throw it into a burning fire to avoid explosions or other accidents.\\n\\nTo maintain aviation safety, please do not use this product during flight. The network signal of this product may cause\\n\\ninterference to the internal navigation systems of planes.<Next> \\nCleaning and maintenance (stand)\\n\\nPlease use a clean dry cloth for cleaning and wiping.\\n\\nDisclaimers\\n\\nThis product manual is provided “as-is” without warranties of any kind, either express or implied. To the maximum extent\\n\\npermitted by applicable law, we provide the document AS IS AND WITH ALL FAULTS, you shall bear all the risks arising from\\n\\nthe use of this manual.\\n\\nYou fully understand and agree that we reserve the right to make changes to this product manual. In the event of changes to\\n\\nproduct specifications or product drivers, this manual will be updated accordingly. For information on detailed updates to this\\n\\nproduct manual, please visit the customer service page on the official pixsee website.\\n\\nPlease refer to the terms of use and privacy policy description page in the app to view software disclaimers for use of the\\n\\npixsee app.\\n\\n25\\n\\nIn-Warranty Repair and Service\\n\\nThis product's warranty repair and service will no long apply should the following occur:<Next> \\nHR DE LU SI CH\\n\\nCY EL MT ES TR\\n\\nCZ HU NL SE\\n\\nDK IE PL UK\\n\\nCompany name: SHENNONA CORPORATION Address: 1171 Montague Expressway Milpitas, CA 95035-6845 Tel number: 408-262-7877\\n\\n23\\n\\nGentle reminder\\n\\nIn order to reduce the risk of Sudden Infant Death Syndrome, CDC (Centers for Disease Control and Prevention) has compiled\\n\\na few tips for what parents and caregivers can do to help babies sleep safely. We hope every baby will sleep safely, warm and\\n\\nsound.\\n\\nBabies sleeping in the same bed with others is one of the risk factors for sudden\\n\\ninfant death syndrome. Even with parents or multiple siblings, it is recommended\\n\\nfor infant to sleep in the same room but on different beds.\\n\\nWhen using a towel or blanket to keep your baby warm, you should wrap it your\\n\\nbaby's chest and expose their arms to reduce the chance of the towel or blanket\\n\\ncovering your baby’s face, or wear a one-piece or sleeping bag-type baby\\n\\npajamas.</context>\"}, {'role': 'user', 'content': 'Can you explain in another way ?'}]\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Can you explain in another way ?\"\n",
    "resposta = interact_with_model(query_text, faiss_index, embeddings, docs_texts, pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e99c4be-2b30-44ac-b78d-0f14391ef37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Based on the information provided, it seems like the manual is for a device that is likely a modem or a router, given the mention of network signal and internet connection. However, there is no clear indication of what the device actually is.\\n\\nTo provide a clear and concise explanation, I\\'ll break down the key points from the manual:\\n\\n1. **Safety Precautions**:\\n   - Keep the device away from towels, blankets, and any part of your body to avoid overheating.\\n   - Avoid placing the device near magnetic materials, flammable gases, or humid conditions.\\n   - Don\\'t use the device during thunderstorms or near fire sources.\\n   - Don\\'t use the device during flights, as it may interfere with navigation systems.\\n\\n2. **Cleaning and Maintenance**:\\n   - Use a clean, dry cloth to wipe the device.\\n\\n3. **Warranty and Disclaimer**:\\n   - The product manual is provided \"as-is\" without warranties.\\n   - You agree to bear all risks arising from using the manual.\\n   - The manufacturer reserves the right to make changes to the manual.\\n\\n4. **Important Reminder**:\\n   - The manual includes a gentle reminder about Sudden Infant Death Syndrome (SIDS) prevention tips, which seem unrelated to the device.\\n\\n'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85266856-710a-4973-bfe4-e7aa4afe7804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What can i do In the event of poor wireless signal ?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"According to the manual, if you're experiencing poor wireless signal, you can try the following:\\n\\n1. Increase the separation between the equipment (pixsee Smart Baby Camera) and the receiver (mobile device).\\n2. Connect the equipment into an outlet on a circuit different from that to which the receiver is connected.\\n3. Consult the dealer or an experienced radio/TV technician for help.\\n\\nAdditionally, the manual also mentions that environmental factors and certain objects can affect the quality of wireless radio communications, such as:\\n\\n- Large objects containing metal (e.g. refrigerators, mirrors, blades, cabinets, metal doors or reinforced concrete)\\n- Other solid objects such as wall structures or other electronics that transmit wireless signals (e.g. televisions, calculators, wireless phones, fluorescent lamps, or dimmer switches)\\n\\nTry moving the equipment away from these objects to see if it improves the signal strength.\"},\n",
       " {'role': 'user', 'content': 'What can i do to clean the equipment ?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'According to the manual, to clean the equipment, you should:\\n\\n1. Turn off the power and disconnect the power cord from the AC adapter.\\n2. Use a clean dry cloth to wipe and clean the equipment.\\n3. Avoid using strong solvents such as thinners, benzene-containing solvents or other chemical solvents next to or on the product.\\n4. Do not use the product in environments where it may be exposed to water splashes, showers, or contact with sea water, salt water, chlorinated water or beverages and other liquids, as this may void the warranty.\\n\\nNote that for the camera, it only features splash-proof water protection, so be cautious when cleaning it.\\n\\nFor the stand, you can also use a clean dry cloth to clean and wipe it.'},\n",
       " {'role': 'user', 'content': 'Can you explain in another way ?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"To clean the pixsee Smart Baby Camera and its stand, follow these steps:\\n\\n1. **Turn it off**: Make sure the camera is turned off and the power cord is disconnected from the AC adapter.\\n2. **Use a dry cloth**: Take a clean, dry cloth and gently wipe down the camera and stand to remove any dust or dirt.\\n3. **Avoid harsh chemicals**: Don't use strong chemicals like thinners, benzene, or other harsh solvents to clean the camera or stand. They can damage the equipment.\\n4. **Be careful with water**: The camera is only splash-proof, so avoid exposing it to water splashes, showers, or contact with water. This can damage the camera and void the warranty.\\n\\nRemember to always handle the equipment with care to avoid any damage.\"}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994fb07-b3bd-4864-8c69-988ddef24bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "029cf521-a7b2-402d-8881-7afafc0a1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gráfico salvo como 'tsne_plot_tooltip.html'. Abra no navegador para visualizar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning:\n",
      "\n",
      "'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "\n",
    "# Configurar o renderer\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "# 1. Garantir que as embeddings dos documentos e da query estejam no formato correto\n",
    "docs_embeddings_np = docs_embeddings  # Renomear para manter consistência\n",
    "query_embedding_np = query_embedding.reshape(1, -1)  # Garantir formato correto\n",
    "\n",
    "# 2. Combinar as embeddings dos documentos e a embedding da query\n",
    "all_embeddings = np.vstack([docs_embeddings_np, query_embedding_np]).astype('float32')\n",
    "\n",
    "# 3. Aplicar t-SNE para reduzir para 2 dimensões\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10, n_iter=500)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# 4. Criar lista de textos (documentos + query)\n",
    "texts = [doc.page_content for doc in docs]  # Documentos\n",
    "query_text_wrapped = \"QUERY: \" + query_text  # Texto específico para a query\n",
    "texts.append(query_text_wrapped)  # Garantir que a query seja a última entrada\n",
    "\n",
    "# 5. Criar DataFrame para visualização\n",
    "data = pd.DataFrame({\n",
    "    \"x\": embeddings_2d[:, 0],  # Primeiro componente t-SNE\n",
    "    \"y\": embeddings_2d[:, 1],  # Segundo componente t-SNE\n",
    "    \"Text\": texts,             # Textos dos documentos e da query\n",
    "    \"Type\": [\"Document\"] * len(docs) + [\"Query\"]  # Tipo: Document ou Query\n",
    "})\n",
    "\n",
    "# 6. Formatar os tooltips para limitar a largura\n",
    "def format_tooltip(text, max_length=50):\n",
    "    \"\"\"Quebra o texto em múltiplas linhas com um limite de caracteres.\"\"\"\n",
    "    return '<br>'.join([text[i:i+max_length] for i in range(0, len(text), max_length)])\n",
    "\n",
    "data[\"FormattedText\"] = data[\"Text\"].apply(lambda x: format_tooltip(x))\n",
    "\n",
    "# 7. Criar gráfico interativo com Plotly\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"Type\",\n",
    "    hover_data={\"FormattedText\": True, \"x\": False, \"y\": False},  # Tooltips formatados\n",
    "    title=\"t-SNE dos Documentos e Query Embeddings\",\n",
    "    labels={\"x\": \"t-SNE Component 1\", \"y\": \"t-SNE Component 2\"}\n",
    ")\n",
    "\n",
    "# Melhorar a aparência dos pontos\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.8, line=dict(width=1, color='DarkSlateGrey')))\n",
    "\n",
    "# 8. Salvar como HTML\n",
    "fig.write_html(\"tsne_plot_tooltip.html\")\n",
    "print(\"Gráfico salvo como 'tsne_plot_tooltip.html'. Abra no navegador para visualizar.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b30ce-0dc9-4a02-8160-7596e82f9887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709133e-aa0b-4593-935d-074df16a1722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb7f06-d9b3-4e72-be4b-0601c58f121f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4bdbd-a48e-4c97-b272-d4f31a330d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7986e-f725-4a7d-b4e9-770facda51de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c8f7e-58b0-40f4-8c97-d42923a3c1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
